\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage[a4paper]{geometry}
\usepackage[french]{babel}
% \usepackage{libertine}
% \usepackage[pdftex]{graphicx}
% \usepackage[pdftex,pdfborder={0 0 0}]{hyperref}



\frenchspacing 
%  \textwidth 16.5cm \textheight
% 24cm \setlength{\oddsidemargin}{0cm}
% \setlength{\evensidemargin}{0cm} \setlength{\topmargin}{-1cm}
% \newtheorem{ex} {Exercice}
% \def \Frac {\displaystyle \frac }
% \def \Sum {\displaystyle \sum }
% \def \Prod {\displaystyle \prod }
% \def \Int {\displaystyle \int }
% \def \N {\mathbb{N}}
% \def \R {\mathbb{R}}
% \def\prob{\hbox{\textit{I\hskip -2pt  P}}}











% \setlength{\parindent}{0cm}
% \setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\begin{document}
\thispagestyle{empty}
\begin{titlepage}
  \begin{sffamily}
  \begin{center}

    % Upper part of the page. The '~' is needed because \\
    % only works if a paragraph has started.
  

    \textsc{\LARGE Université de Lille 3}\\[2cm]

    \textsc{\Large Rapport de TER }\\[1.5cm]

    % Title
    \HRule \\[0.4cm]
    { \huge \bfseries Nettoyage d'une Base de données\\[0.4cm] }

    \HRule \\[2cm]


    % Author and supervisor
    \begin{minipage}{0.4\textwidth}
      \begin{flushleft} \large
        DEREGNAUCOURT Paul\\
        POULAIN Romain\\
      \end{flushleft}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
      \begin{flushright} \large
        \emph{Tuteur :} M. \textsc{STAWORKO}

      \end{flushright}
    \end{minipage}

    \vfill

    % Bottom of the page
    {\large  Octobre 2019 — Mai 2020}

  \end{center}
  \end{sffamily}
\end{titlepage}
 

\section{Introduction}
\thispagestyle{empty}

Les problèmes liés au nettoyage de données sont apparus au début des années 2000 avec l'explosion d'Internet et des entrepôts de données. A cause du volume croissant de données, il y a eu un problème dans les qualités des données des différentes bases.


La qualité des données correspond à la conformité des données aux usages prévus. En effet la qualité des données est très utile pour une entreprise. Elle lui permet évidemment de gagner de l'argent. Par exemple une entreprise comme Free collecterait les données de numéro de téléphone de ses clients. Ces données seraient utilisées dans les forfaits téléphoniques de leurs clients. Mais ces données seraient de mauvaise qualité suite à un problème de détection des antennes. L'entreprise récupèrerait donc de mauvais numéros de téléphone. Ce serait un très gros problème car ils pourraient perdre des millions d'euros car les numéros de téléphone dans la base de données seraient de mauvaise qualité or l'entreprise de téléphonie mobile aurait besoin des bons numéros de téléphone associés aux clients (Par exemple un appel d'une 1 heure est décompté sur une personne avec un forfait bloqué donc il y a des frais téléphoniques que Free comptabilise pour la mauvaise personne). Pour réparer ces erreurs il faut donc débourser beaucoup d'argent et il faut également nettoyer la base de données qui est de mauvaise qualité. C'est un gros coup dur pour les entreprises. La qualité de données permet donc également de gagner du temps parce que si les données sont de mauvaise qualité, il faudra améliorer la qualité pour que le travail de l'entreprise se fasse dans de bonnes conditions et qu'il n'y a pas à réparer les erreurs que peuvent engendrer beaucoup de perte de temps et d'argent.

Il peut y avoir plusieurs problèmes de données qui affectent la qualité de données. 
Premièrement il y a des problèmes de syntaxe dans l'écriture de données. Par exemple mettre l'âge d'une personne dans la colonne du sexe de la personne, des erreurs de formatage avec des données qui ne sont pas du même format. Il y a aussi des problèmes d'irrégularité par exemple des distances exprimées en kilomètres mais des données sont ecrie en miles.
Deuxièmement il y a des problèmes de sémantiques.
Elles concernent notamment une violation des contraintes d'intégrité de la base, par exemple une personne avec un âge négatif.
Il y a des problèmes de contracdiction, par exemple l'âge ne correspond pas avec la date de naissance.
Il y a des problèmes de qualité de données de duplication, c'est à dire plusieurs données relatives à une même données.
Et enfin des problèmes de données invalides.
Dernièrement il y a des problèmes de qualité de données de couverure.
comme valeurs manquantes, par exemple il manque la date de naissance d'un personne ou données manquantes, par exemple aucun âge est spécifé dans la base de données.


Dans notre cas nous allons nous concentrer sur l'erreur de duplication des données.
Principalement, on parle des doublons, ce sont des erreurs qui représentent deux fois le même élément.  On estime environ à $ 25\% $  des données critique.  De plus, $ 2\% $ des enregistrements dans un fichier client peuvent devenir obsolètes en un mois (changement d’adresse, de numéro), soit environ $50\%$ de données erronées dans un fichier en 2 ans. Ceci est très dérangeant car des données sales coutent excrément cher.
Imaginons des clients dans une banque qui aurait des identités en double ou encore des envoies de catalogues en double ou à une mauvaise adresse, il faudrait alors renvoyer le catalogue à la bonne adresse, ce qui à pour conséquence une perte de temps et d'argent.
Il existe un site qui recueille toutes sortes de livres, page web, télévisions. Il s’agit Open Library Data Dumps basée à San Franciso, en Californie avec des satellites dans le monde entier.  Ceci à pour but de rendre toutes les œuvres publiées de l’humanité accessibles à tous dans le monde afin de construire un accès universel à toutes les connaissances.
Dans ce projet, nous travaillerons sur une base de données qui constitue des livres, des auteurs.



  


Cependant la base de données Open Library Data Dumps est impure, ce qui veut dire qu’elle n’est pas nettoyée et qu’elle possède de nombreuses données incohérentes, de doublons et de fausses informations.
La détection automatique des doublons est difficile : premièrement, les représentations en double ne sont généralement pas identiques mais légèrement différentes dans leurs valeurs. Deuxièmement, en principe, toutes les paires d'enregistrements devraient être comparé, ce qui est impossible pour de gros volumes de données.



Pour parvenir à nettoyer notre base de données Open Library Data Dumps, nous allons procéder en 3 étapes pour effectuer la déduplication de nos données:

\begin{enumerate}
\item \textbf{Identification des doublons.} Pour réaliser ce projet, nous allons commencer par étudier la base de données, pour cela nous aurons besoins de rendre la base exploitable, car celle-ci est sous format json, ce qui n’est pas pratique pour l’exploiter afin de la nettoyer. Une fois que nous aurons la base souhaités, sous formes de tables exploitables alors nous pourrons commencer à l’étudier.
Tout d’abord, nous allons  étudier les doublons de notre base de données. En effet, cette base en contient, notamment quand il s'agit des titres de livres ou des noms et prénoms d' auteurs. Donc le but de notre projet est de faire la déduplication de nos doublons. C’est un moyen de stockage de données qui consiste à  factoriser des séquences de données identiques pour libérer de l’espace dans la base de données. Donc ici nos doublons seront analysés élément par élément pour savoir s’ils sont suffisamment proches pour être considéré comme le même objet.
Pour cela, nous allons utiliser des fonctions de similarités qui auront pour but de comparer les objets entre eux avec un certain seuil de similitude pour déterminer si 2 objets sont considérés comme similaires. Par exemple pour allons prendre la distance entre deux mots.(John et Johny par exemple).



\item \textbf{Clustering.}
Si nous obtenons des données similaires nous allons les placer dans ce qu’on appelle un graphe de similitude. Tous les objets qui auront un seuil suffisamment proche seront clusturisé, c’est-à-dire qu’on va les regroupés ensemble et ils seront proches dans le graphe de similitude. Ensuite de par ce  graphe nous pourrons déterminer si les groupes d’objets sont suffisamment similaires pour être considéré comme des doublons.


\item \textbf{Fusion des clusters.} Si c’est le cas nous procéderons à la fusion de nos groupes (clusters). En effet nous allons fusionner les doublons pour que ces groupes d’objets similaires ne soient plus qu’un seul et unique objet. C'est après cette étape que nous allons commencé à nettoyer notre base pour qu'elle soit propre. 


\end{enumerate}


Dans un premier temps nous allons étudier l'implémentation de Library Open Data Dumps, en format json, dans une base de données puis prendre connaissance de cette dernière et nous allons identifier les doublons notamment avec des fonctions de similitudes.
Dans un deuxième temps nous allons etiqueter ces doublons, puis procéder à leur regroupement (clustering) dans un graphe de similitude.
Dans un dernier temps nous allons fusionner nos groupes (clusters) pour otenir une base de donnée nettoyée.


\section{Récupération et implementation de fichier json dans une base de données}
\subsection{Implementation dans une base phpmyadmin}
La première tâche à effectuer était de récupérer la base de données Open Library Data Dumps en format json sur https://openlibrary.org/developers/dumps.Pour cela nous avons téléchargé la base de données puis nous avons pris une petite partie de la base (la base entière fait Ensuite nous avons inséré ces données qui sont contenues dans un fichier text de plus de 42go) grâce à la commande "head" de bash qui permet de récupérer seulement les premières lignes d'un fichier text. Nous avons choisi de travailler au départ avec un échantillon de 100 lignes. Nous voulions donc importer cet échantillon dans une base de données phpmyadmin grâce à nos différents programmes bash que l'on a créé.
Notre programme principal consiste à analyser le fichier text, (qui contient du json) ligne par ligne pour repérer et récupérer les différents éléments de la base de données json. Une fois ces données repérées, nous les séparons pour avoir nos différents attributs : name, personal\_name, last\_modified et révision. Grâce à cela nous avons pus récupérer les données de chaque attribut. Ensuite nous avons créé la base de données "authors" dans phpmyadmin avec comme champs ses différents attributs. Comme nous avons isolé ces derniers, nous pouvons les ajouter dans notre base de données grâce aux requêtes SQL INSERT INTO. C'est comme cela que nous avons notre première base de données avec les données issues d'Open Library Data Dumps.
Nous avons eu des difficultés à savoir comment extraire les données d'un fichier json pour les mettre dans une base de données. Nous avons réussi à résoudre cela en nous documentant et en effectuant des recherches.
Evidemment ceci n'est qu'une première version qui était loin d'être optimal. Nous avons donc cherché à extraire et à implémenter ces données autrement notamment avec Python.

\subsection{Implementation dans une base SQLite avec Python}
Notre premier objectif était de réaliser la même importation qu'avec nos programmes bash mais avec Python. Mais nous avons vite compris qu'il fallait procéder autrement. Nous avons tout d'abord installé les différents composants de SQLite sur notre ordinateur, que l'on utilisera comme base de données. Nous avons cherché pendant un moment pour ne plus récupérer les données manuellement en analysant les lignes une par une mais en automatisant cette recherche  et récupération de données json. Après de nombreuses recherches, nous avons décidé d'importer et d'utiliser la librairie de sqlite sur python. Nous avons d'abord appris à utiliser cette librairie avec des données fictives que nous avons créées pour nous entrainer à se servir de cette dernière. Une fois que l'on comprit comment fonctionnait l'importation sqlite de python, nous pouvions commencer à utiliser les vraies données json issues d' Open Library Data Dumps. Nous avons compris que la base de données de Library Data Dumps était séparée en trois tables. La première est Authors, qui concerne les auteurs des livres, sur lequel nous allons effectuer notre grande partie du projet, puis les deux autres bases de données que sont Works, qui est la table concernant les livres et enfin Editions qui concerne l'edition des livres. Nous avons donc téléchargé les trois tables séparément et nous avons créé comme pour nos programmes bash, des échantillons de 1000 lignes de chaque table pour pouvoir extraire les données json et les insérer dans notre base de données sqlite.

A l'aide de la documentation officielle du site Open Library Data Dumps, nous créeons 3 tables de base de données sql, une author, une work et une edition. Nous avons tout d'abord analysé la documentation du site pour savoir tous les attributs disponibles pour nos différentes tables. Nous les avons mis dans leurs tables respectives et nous pouvons créer les tables grâce aux commandes sql "CREATE TABLE". Une fois la table créée, il fallait donc ajouter les données correspondantes dans chaque table. Pour cela nous utilisons le package sqlite de python utiliser précédemment. Pour récupérer tous les attributs dans le json, nous utilisons json.load de python. Grâce à la fonction "get" nous pouvons récupérer le contenu de chaque attribut et donc les mettre dans notre base de données. 

Nous avons eu des difficultés pour récupérer certains types de données. En effet, certaines données sont plutôt "simples" à récupérer car il suffit juste d'utiliser la fonction get de json. Mais de nombreuses données sont bien plus complexes que cela. Par exemple l'attribut "bio" nous a causé de nombreux soucis car le type de cet attribut faisait planter notre programme, notamment notre programme initial avec bash. En effet en effectuant des recherches, l'attribut bio, comme d'autres attributs, est un dictionnaire. Donc la fonction get retourne une erreur dans le cas des dictionnaires. Nous avons donc dût isoler ces attributs pour pouvoir les traiter. En effet si le programme arrive sur bio ou plus généralement sur un dictionnaire, alors il récupére l'attribut "value" du dictionnaire. Cette dernière contient la valeur de la bio, c'est donc ce que nous voulons. Ensuite les autres attributs où nous avons rencontré des difficultés sont les tableaux. En effet si nous récupérons des tableaux avec la fonction get, nous obtenons que la dernière valeur du tableau, les autres sont écrasés par cette dernière. Il fallait donc un moyen pour avoir toutes les informations possibles. Pour cela nous avons fait un traitement avant la requête sql INSERT INTO. Chaque fois qu'il y a un attribut de type tableau, nous ajoutons ces données dans un nouveau tableau que nous convertissons en chaine de caractère pour que la requête sql fonctionne. Ensuite nous mettons dans notre requete sql notre nouvelle chaine de caractère.Nous avons donc toutes les données des attributs de type tableau. Enfin le dernier point qui nous a posé problème pour l'importation des données dans notre base est les attributs qui ont un type prédéfini qui leur est propre. En effet tous les attributs ne sont pas du type entier, chaine de caractère, tableaux ou dictionnaire, il existe des types uniques. Ces types possèdent également des attributs. Pour résoudre cela nous avons procédé de la même manière que pour les tableaux sauf qui nous avons utilisé des dictionnaires. En effet à chaque attribut du type, nous créons un dictionnaire pour avoir toutes les données, puis nous ajoutons ce dictionnaire dans un tableau, que nous convertissons en chaine de caractère. Ce procéder nous permet de récupérer chaque attribut du type et donc de récupérer toutes les données. C'est le point qui nous a posé le plus de souci. Une fois que nous avons réussi à extraire toutes nos données json de la base de données Open Libray Data Dumps, nous exécutons notre requête sql avec les instructions INSERT INTO pour ajouter nos données dans nos différentes tables. C'est ainsi que toutes les données sont dans notre base.

\section{Distance d'édition entre les autheurs}

Pour pouvoir nettoyer la base de données Open Library Data Dumps, il faut dans un premier temps faire le repérage des doublons. Cette phase va nous permettre de faire la déduplication des données dans notre base. Dans notre cas, nous nous concentrons sur les données des auteurs, dans la table authors donc. Il s'agit de calculer l'edit-distance entre deux nombres, notion vue dans  An introduction to Duplicate Detection \cite{10.5555/1841211}.

\subsection{Distance d'édition avec des dictionnaires}
Notre première version pour pouvoir identifier les doublons est de faire l'edit-distance des noms des auteurs. Cela signifie que l'on va comparer la différence de caractère entre deux chaines de caractères. Par exemple le nom Jhon aura 1 de différence avec le nom john car les lettres 'h' et 'o' sont inversées. A chaque lettre de différence, la valeur de l'edit-distance sera incrémenté de 1. Nous allons donc utiliser cette technique pour comparer les noms des auteurs et donc pour savoir s'il y a des auteurs en double. Pour notre première version nous avons utilisé le package 'nltk' de python qui permet de calculer l'edit-distance entre deux nombres. Ensuite pour avoir tous les noms des auteurs de notre table authors, nous effectuons une requête sql Select qui récupère tous les noms des auteurs. Puis on utilise des dictionnaires pour comparer chaque nom avec chaque autre nom d'auteurs. Ces dictionnaires nous permettent d'effectuer toutes les comparaisons possibles entres chaque nom. Les noms nous les récupérons grâce à une requête sql avec le package sqlite de python. Une fois les noms récupérés, nous parcourons notre liste de noms pour pouvoir traiter tous les noms possibles. Ensuite nous calculons la distance d'édition entre tous les noms puis nous les affichons. Cette première version était un peu complexe a réalisé car il fallait comprendre le principe des dictionnaires, mais au final elle était fonctionnelle. Il était tout de fois pas optimisé et c'est pour cela que nous changeons notre manière de calculer la distance d'édition des noms des auteurs.


\subsection{Distance d'édition avec des fonctions}
Nous avons compris que l'edit-distance avec des dictionnaires n'était pas optimisée. C'est pour cela que nous avons décidé de faire des différentes fonctions qui puissent optimiser notre code. Pour cela nous créons 3 fonctions principales: la fonction dist\_name qui prend en paramètre deux identifiants d'auteurs à comparer. Elle calcule la distance d'édition des noms des auteurs. Ensuite nous avons une fonction dist\_date qui prend les mêmes paramètres que la fonction pour les noms. Elle calcule la distance d'édition des dates de naissances et des dates de décès des auteurs. Et enfin nous avons une fonction dist\_author qui prend les mêmes paramètres et qui, avec les deux fonctions permet de renvoyer une distance d'édition globale de l'auteur. Avec cette distance d'édition globale, nous pouvons déterminer un certain seuil à ne pas dépasser pour savoir si deux auteurs sont des doublons ou non. Le rôle de la fonction dist\_name est de comparer la distance d'édition des noms des authors grâce à la fonction nltk utilisée précedemment. On récupère les noms des auteurs grâce à une fonction annexe qui effectue une requête sql pour récupérer les noms des auteurs directement dans la base de données. La fonction dist\_date permet de renvoyer la distance d'édition des dates de naissances et de décès des auteurs selon les données disponibles. Comme pour les noms, nous récupérons les dates de naissances et de décès des auteurs grâce aux fonctions qui effectuent des requêtes sql permet de récupérer ces dernières. La grosse difficulté des dates, hormis le fait qu'il y a de nombreuses valeurs nulles, est que toutes les dates ne sont pas dans le même format. Pour cela nous avons réalisé une fonction annexe qui convertit toutes les dates en format année. Enfin la fonction dist\_author permet d'avoir une comparaison globale des auteurs et c'est à partir de cette fonction que nous allons déterminer le seuil qui nous permettra de savoir si deux auteurs sont similaires, donc potentiellement doublon ou non.

Les difficultés que nous avons rencontrées sont surtout liés à comment on découpe notre code pour faire différentes fonctions effectuant un traitement optimisé comparer aux dictionnaires précédemment. Ensuite nous avons plusieurs problèmes notamment dans la gestion des valeurs nulles (None) que nous renvoyer nos fonctions. Nous avons réussi à régler ces problèmes grâce notamment à de nombreuses conditions gérant les cas de nullité.


\subsection{Conclusion}

\cite{Fan15}
\cite{galhardas:inria-00072476}
\cite{10.5555/1841211}

\bibliographystyle{plain}
\bibliography{ter}

\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End: